{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i5UffaGR1EfD"
   },
   "source": [
    "# Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JfMc3OKphLsO"
   },
   "outputs": [],
   "source": [
    "# mount Google Drive every time you restart instance\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "# set working path every time after mounting drive or set alternative path\n",
    "root_path = 'gdrive/My Drive/Dataset/'\n",
    "cityscapes_path = root_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f7NdV9_rx766"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YY2SsKqD-P0W"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "from distutils.version import LooseVersion\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import cv2\n",
    "import os, sys, time\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils import data\n",
    "from typing import Tuple\n",
    "from torchvision import transforms, utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from skimage import io, transform\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from torchvision import models\n",
    "from torchvision.models.vgg import VGG\n",
    "from PIL import Image\n",
    "from albumentations import RandomContrast, RandomBrightness, Compose, RandomCrop, HorizontalFlip, Normalize\n",
    "\n",
    "# import files for labels and models\n",
    "%run 'gdrive/My Drive/Colab Notebooks/labels.py'\n",
    "%run 'gdrive/My Drive/Colab Notebooks/models.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rF8fNAFBvnAX"
   },
   "source": [
    "# Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fc_eAY3F-UkB"
   },
   "outputs": [],
   "source": [
    "# Set username and password for Datascapes account. Download images\n",
    "# This code block runs only once to create dataset. Username and password have been removed. \n",
    "\n",
    "!wget --keep-session-cookies --save-cookies=cookies.txt --post-data 'username=XXXXXX&password=XXXXX&submit=Login' https://www.cityscapes-dataset.com/login/\n",
    "!wget --load-cookies cookies.txt --content-disposition https://www.cityscapes-dataset.com/file-handling/?packageID=3\n",
    "!wget --load-cookies cookies.txt --content-disposition https://www.cityscapes-dataset.com/file-handling/?packageID=1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tLABKQLC-XNT"
   },
   "outputs": [],
   "source": [
    "# Create folders and unzip dataset\n",
    "\n",
    "!mkdir dataset && mkdir dataset/gtFine_trainvaltest && mkdir dataset/leftImg8bit_trainvaltest\n",
    "!unzip gtFine_trainvaltest.zip -d dataset/gtFine_trainvaltest\n",
    "!unzip leftImg8bit_trainvaltest.zip -d dataset/leftImg8bit_trainvaltest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_JWayeH26Tqg"
   },
   "outputs": [],
   "source": [
    "def image_resize(path: str, mode: str):\n",
    "    \"\"\"Resise images to half size\n",
    "    \n",
    "    Args:\n",
    "        path (str): Path to images\n",
    "        mode (str): \"image\" or \"annotation\". The mode determines the type of interpolation and colour/greyscale\n",
    "    \"\"\"\n",
    "    \n",
    "    assert mode in [\"image\", \"annotation\"], \"Unsupported mode\"\n",
    "    \n",
    "    images = list(Path(path).glob('**/*.png'))    \n",
    "    \n",
    "    if mode == \"image\":\n",
    "         for image in images:\n",
    "            oriimg = cv2.imread(str(image), cv2.IMREAD_COLOR)\n",
    "            newimg = cv2.resize(oriimg, None, fx=0.5,fy=0.5, interpolation = cv2.INTER_AREA)\n",
    "            cv2.imwrite(str(image), newimg)\n",
    "    elif mode == \"annotation\":\n",
    "        for image in images:\n",
    "            oriimg = cv2.imread(str(image), cv2.IMREAD_GRAYSCALE)\n",
    "            newimg = cv2.resize(oriimg, None, fx=0.5,fy=0.5, interpolation = cv2.INTER_NEAREST)\n",
    "            cv2.imwrite(str(image), newimg)\n",
    "\n",
    "# resize images      \n",
    "image_resize(\"/content/dataset/leftImg8bit_trainvaltest/\", mode=\"image\")\n",
    "\n",
    "# resize annotations\n",
    "image_resize(\"/content/dataset/gtFine_trainvaltest/\", mode='annotation')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4ue9A1JlcSWO"
   },
   "outputs": [],
   "source": [
    "# copy resized images to Google Drive folder\n",
    "\n",
    "!cp -r dataset/ gdrive/My\\ Drive/Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z7udhvL04p9v"
   },
   "source": [
    "# Image Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nOeI62v3_Zz0"
   },
   "outputs": [],
   "source": [
    "# image transformations\n",
    "\n",
    "# crop should be changed to 1024 if original sized imaged are to be used \n",
    "\n",
    "trans_crop = RandomCrop(p=1, height=512, width=512)\n",
    "trans_flip = HorizontalFlip(p=0.5)\n",
    "trans_contrast = RandomContrast(p=0.1)\n",
    "trans_brightness = RandomBrightness(p=0.1)\n",
    "trans_norm = Normalize(p=1)\n",
    "\n",
    "# augmentations for training\n",
    "aug = Compose([trans_crop, trans_flip, trans_contrast, trans_norm])\n",
    "\n",
    "# augmentations for testing - only normalisation\n",
    "aug_test = Compose([trans_norm])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JtT5pMHWgRW3"
   },
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nHl_2TU_Coj_"
   },
   "outputs": [],
   "source": [
    "def labels2rgb(labels: np.array, lut: np.array) -> np.array:\n",
    "    \"\"\"Helper function which prints coloured image rediction\n",
    "    \n",
    "    Args:\n",
    "        labels (Numpy Array): Labels\n",
    "        lut (Numpy Array): Colours\n",
    "    Returns:\n",
    "        im (Numpy Array): Colour image\n",
    "    \n",
    "    \"\"\"\n",
    "    im = cv2.LUT(cv2.merge((labels, labels, labels)), lut)\n",
    "    im = cv2.cvtColor(im, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tUvmkZLI_c2z"
   },
   "outputs": [],
   "source": [
    "def one_hot(anot, class_num: int) -> np.array:\n",
    "    \"\"\"Returns one hot encoded images\n",
    "    \n",
    "    Args:\n",
    "        anot: Annotations\n",
    "        class_num(int): Number of classes\n",
    "    Returns: \n",
    "        One hot encoded array (Numpy Array)\n",
    "    \"\"\"\n",
    "    \n",
    "    eye_tb = np.eye(class_num)[np.array(anot).reshape(-1)]\n",
    "    return eye_tb.reshape(list(anot.shape)+[class_num])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1KZq-OXpxQdw"
   },
   "source": [
    "# Cityscape Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3FYajChd_fiR"
   },
   "outputs": [],
   "source": [
    "class CityscapeDateset(Dataset):\n",
    "    \"\"\"Cityscape dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, mode = \"train\",transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.images_path = os.path.join(root_dir ,\"leftImg8bit_trainvaltest\")\n",
    "        self.annotations_path = os.path.join(root_dir , \"gtFine_trainvaltest\")\n",
    "\n",
    "        self.imgs_train = list(Path(self.images_path+'/leftImg8bit/train').glob('**/*.png'))\n",
    "        self.imgs_test = list(Path(self.images_path+'/leftImg8bit/test').glob('**/*.png'))\n",
    "        self.imgs_val = list(Path(self.images_path+'/leftImg8bit/val').glob('**/*.png'))\n",
    "        self.imgs_train = sorted(self.imgs_train)\n",
    "        self.imgs_test = sorted(self.imgs_test)\n",
    "        self.imgs_val = sorted(self.imgs_val)\n",
    "        \n",
    "        self.annotations_train = list(Path(self.annotations_path+'/gtFine/train').glob('**/*gtFine_labelIds.png'))\n",
    "        self.annotations_test = list(Path(self.annotations_path+'/gtFine/test').glob('**/*gtFine_labelIds.png'))\n",
    "        self.annotations_val = list(Path(self.annotations_path+'/gtFine/val').glob('**/*gtFine_labelIds.png'))\n",
    "        self.annotations_train = sorted(self.annotations_train)\n",
    "        self.annotations_test = sorted(self.annotations_test)\n",
    "        self.annotations_val = sorted(self.annotations_val)\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.mode == \"train\":\n",
    "            return len(self.imgs_train)\n",
    "        elif self.mode == \"test\":\n",
    "            return len(self.imgs_test)\n",
    "        elif self.mode == \"val\":\n",
    "            return len(self.imgs_val)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode == \"train\":\n",
    "            img_name = self.imgs_train[idx]\n",
    "            annotation_name = self.annotations_train[idx]\n",
    "  \n",
    "        elif self.mode == \"test\":\n",
    "            img_name = self.imgs_test[idx]\n",
    "            annotation_name = self.annotations_test[idx]\n",
    "\n",
    "        elif self.mode == \"val\":\n",
    "            img_name = self.imgs_val[idx]\n",
    "            annotation_name = self.annotations_val[idx]\n",
    "\n",
    "        \n",
    "        image = io.imread(img_name)\n",
    "        annotations = io.imread(annotation_name)\n",
    "\n",
    "        if self.mode == \"train\":\n",
    "            augmented = aug(image=image, mask=annotations)\n",
    "          \n",
    "            image = torch.from_numpy(augmented['image']).float().permute(2, 0, 1)\n",
    "\n",
    "            annotations = augmented['mask']\n",
    "            w,h = annotations.shape\n",
    "            annotations = annotations.reshape(w*h,)\n",
    "            annotations = one_hot(annotations, 34)\n",
    "            annotations = annotations.reshape(w,h,34)\n",
    "\n",
    "            sample = {'image':image , 'annotation': annotations} \n",
    "        else:\n",
    "            augmented = aug_test(image=image, mask=annotations)\n",
    "            image = torch.from_numpy(augmented['image']).float().permute(2, 0, 1)\n",
    "            sample = {'image':image , 'annotation': annotations, \"name\": img_name}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9d-UGC9X_ico"
   },
   "outputs": [],
   "source": [
    "# Defining training and test sets\n",
    "# validation set used as test set \n",
    "\n",
    "train_db = CityscapeDateset(cityscapes_path)\n",
    "test_db = CityscapeDateset(cityscapes_path, mode = \"val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XvDxGUyv9xFn"
   },
   "source": [
    "# Model/Mode Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xhZxY-7OR7UP"
   },
   "outputs": [],
   "source": [
    "# model_selection function asks the user to select the model to be trained \n",
    "\n",
    "# 1. FCN32 with ResNet50 backbone\n",
    "# 2. FCN32 with ResNet18 backbone\n",
    "# 3. FCN16 with VGG backbone\n",
    "# 4. FCN8 with VGG backbone\n",
    "# 5. SegNet\n",
    "\n",
    "while True:\n",
    "  selected_model = int(input(\"\"\"Please select model:\n",
    "\n",
    "  1. FCN32 with ResNet50 backbone\n",
    "  2. FCN32 with ResNet18 backbone\n",
    "  3. FCN16 with VGG backbone\n",
    "  4. FCN8 with VGG backbone\n",
    "  5. SegNet \\n\n",
    "  \"\"\"))\n",
    "\n",
    "  if selected_model == 1:\n",
    "      backbone = models.resnet50(pretrained=True)\n",
    "      backbone = torch.nn.Sequential(*(list(backbone.children())[:-2]))\n",
    "      model = FCN32s(backbone, 34, n_feats_backbone=2048).float().cuda()\n",
    "      model_name = \"fcn32_resnet50\"\n",
    "      print(\"\\nFCN32 with ResNet50 backbone selected.\")\n",
    "      break\n",
    "  elif selected_model == 2:\n",
    "      backbone = models.resnet18(pretrained=True)\n",
    "      backbone = torch.nn.Sequential(*(list(backbone.children())[:-2]))\n",
    "      model = FCN32s(backbone, 34, n_feats_backbone=512).float().cuda()\n",
    "      model_name = \"fcn32_resnet18\"\n",
    "      print(\"\\nFCN32 with ResNet18 backbone selected.\")\n",
    "      break\n",
    "  elif selected_model == 3:\n",
    "      vgg_model = VGGNet()\n",
    "      model = FCN16s_VGG(pretrained_net=vgg_model, n_class=34).float().cuda()\n",
    "      model_name = \"fcn16_vgg\"\n",
    "      print(\"\\nFCN16 with VGG backbone selected.\")\n",
    "      break\n",
    "  elif selected_model == 4:\n",
    "      vgg_model = VGGNet()\n",
    "      model = FCN8s_VGG(pretrained_net=vgg_model, n_class=34).float().cuda()\n",
    "      model_name = \"fcn8_vgg\"\n",
    "      print(\"\\nFCN8 with VGG backbone selected.\")\n",
    "      break\n",
    "  elif selected_model == 5:\n",
    "      model = segnet(3,34).float().cuda()\n",
    "      model_name = \"segnet\"\n",
    "      print(\"\\nSegNet network selected.\\n\")\n",
    "      break\n",
    "  else:\n",
    "      print(\"Please select a valid model number\\n\")\n",
    "\n",
    "# mode selection\n",
    "\n",
    "while True:\n",
    "  mode_sel = int(input(\"\"\"Please select mode:\n",
    "\n",
    "  1. Training\n",
    "  2. Testing/Evaluating\\n\n",
    "  \"\"\"))\n",
    "  if mode_sel == 1:\n",
    "      print(\"\\nProceed to training...\")\n",
    "      break\n",
    "\n",
    "  elif mode_sel == 2:\n",
    "    model_path = os.path.join(cityscapes_path, model_name + \".pt\")\n",
    "\n",
    "    if os.path.exists(model_path) == True:\n",
    "      checkpoint = torch.load(model_path)\n",
    "      model.load_state_dict(checkpoint['model_state_dict'])\n",
    "      break\n",
    "    else:\n",
    "      print(\"Saved model \\\"\",model_name,\"\\\" not found. Please train model first.\", sep='')\n",
    "  else:\n",
    "    print(\"Please select a valid mode number\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CTJsCgkT2Gr9"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H5FNtmHWZsSP"
   },
   "outputs": [],
   "source": [
    "model_path = os.path.join(cityscapes_path, model_name + \".pt\")\n",
    "\n",
    "# define loss function and optimiser\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# model trained for 20 epochs. modify as appropriate\n",
    "max_epoch = 19\n",
    "\n",
    "dataloader = DataLoader(train_db, batch_size=4, shuffle=True, num_workers=4)\n",
    "\n",
    "if os.path.exists(model_path) == True:\n",
    "  checkpoint = torch.load(model_path)\n",
    "  start_epoch = int(checkpoint['epoch'])\n",
    "  model.load_state_dict(checkpoint['model_state_dict'])\n",
    "  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "  loss = checkpoint['loss']\n",
    "  \n",
    "  if start_epoch < max_epoch:\n",
    "    print(\"Loading epoch %d. Training resuming...\" %start_epoch)\n",
    "  else:\n",
    "    pring(\"Network fully trained\")\n",
    "else:\n",
    "  print(\"No checkpoint found. Training starting...\")\n",
    "  start_epoch = 0\n",
    "\n",
    "for epoch in range(start_epoch, max_epoch):\n",
    "  for i_batch, sample_batched in enumerate(tqdm(dataloader)):\n",
    "      model.train()\n",
    "      images = sample_batched['image'].float().cuda()\n",
    "      gt = sample_batched['annotation'].permute(0,3,1,2).float().cuda()\n",
    "      predictions = model(images)\n",
    "      loss = criterion(predictions, gt)\n",
    "\n",
    "      # Before the backward pass zero all gradients\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "      loss.backward()\n",
    "\n",
    "      # Calling the step function on an Optimizer makes an update to its parameters\n",
    "      optimizer.step()\n",
    "\n",
    "  print(\"Loss: %f\" %loss.item())\n",
    "  print(\"Epoch: %d\" %epoch)\n",
    "\n",
    "  torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss\n",
    "}, model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8ThwbJhf96d2"
   },
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "O5oxvJm_x_NL",
    "outputId": "352b5568-2771-4fe6-b438-fd325f137549"
   },
   "outputs": [],
   "source": [
    "print(\"Selected model: \", model_name, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TI08sDue9ll7"
   },
   "outputs": [],
   "source": [
    "# go into evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# select a random image\n",
    "sample = test_db[2]\n",
    "\n",
    "img = sample[\"image\"].unsqueeze(0)\n",
    "prediction = model(img.cuda())\n",
    "\n",
    "# checks tensor dimensions for original image and predictions\n",
    "print(\"Image tensor:  \", img.size())\n",
    "print(\"Prediction tensor: \", prediction.size())\n",
    "\n",
    "prediction = torch.nn.functional.softmax(prediction,1)\n",
    "y = torch.argmax(prediction, dim=1)\n",
    "\n",
    "#now visualise all of them\n",
    "labels2color = np.zeros((256, 3), dtype=np.uint8)\n",
    "\n",
    "for lbl in labels:\n",
    "    labels2color[lbl.id] = np.array(lbl.color)\n",
    "labels2color = np.expand_dims(labels2color,0)\n",
    "\n",
    "y = y.squeeze().cpu().numpy().astype(np.uint8)\n",
    "\n",
    "# check if the size of the prediction is correct\n",
    "print(\"Size of sample predicted image:\", y.shape)\n",
    "\n",
    "path = sample['name']\n",
    "#print(path)\n",
    "\n",
    "# read original image\n",
    "original_img = cv2.imread(str(path))\n",
    "\n",
    "# generate coloured prediction\n",
    "cityscapes_rgb = labels2rgb(y, labels2color)\n",
    "\n",
    "# write original and annotation images \n",
    "cv2.imwrite(os.path.join(cityscapes_path + model_name + \"_original.png\"), original_img)\n",
    "cv2.imwrite(os.path.join(cityscapes_path + model_name + \"_masks.png\"), cityscapes_rgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Bin2zmYJIX0"
   },
   "source": [
    "# Testing / **Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GnrdYHQnJK89"
   },
   "outputs": [],
   "source": [
    "def intersectionAndUnion(\n",
    "    imPred: np.array, imLab: np.array, numClass: int\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"Calculates Intersection over Union\n",
    "    \n",
    "    Args:\n",
    "        imPred (np.array): Array of predicted images\n",
    "        imLab (np.array): Labeled image\n",
    "        \n",
    "    Returns:\n",
    "        area_intersection, area_union (float, float): Tuple with metrics\n",
    "    \"\"\"\n",
    "    imPred = np.asarray(imPred).copy()\n",
    "    imLab = np.asarray(imLab).copy()\n",
    "\n",
    "    imPred += 1\n",
    "    imLab += 1\n",
    "    # Remove classes from unlabeled pixels in gt image.\n",
    "    # We should not penalize detections in unlabeled portions of the image.\n",
    "    imPred = imPred * (imLab > 0)\n",
    "\n",
    "    # Compute area intersection:\n",
    "    intersection = imPred * (imPred == imLab)\n",
    "    (area_intersection, _) = np.histogram(\n",
    "        intersection, bins=numClass, range=(1, numClass))\n",
    "\n",
    "    # Compute area union:\n",
    "    (area_pred, _) = np.histogram(imPred, bins=numClass, range=(1, numClass))\n",
    "    (area_lab, _) = np.histogram(imLab, bins=numClass, range=(1, numClass))\n",
    "    area_union = area_pred + area_lab - area_intersection\n",
    "\n",
    "    return (area_intersection, area_union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6IaPIlrrRsm2"
   },
   "outputs": [],
   "source": [
    "# go into evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# create tables to store scores IoUs and IoUs per class\n",
    "scores = []\n",
    "ious = []\n",
    "means = []\n",
    "\n",
    "#initialize with NaNs. NaNs will be present when a class does not exist in a picture and then ignored in calculations\n",
    "labels_array = np.empty((34,500)) * np.nan \n",
    "\n",
    "# counter for IoU per class array\n",
    "j = 0 \n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# predict all pictures\n",
    "for sample in test_db:\n",
    "  img = sample[\"image\"].unsqueeze(0)\n",
    "  gt_labels = sample[\"annotation\"]\n",
    "  prediction = model(img.cuda())\n",
    "  prediction = torch.nn.functional.softmax(prediction,1)\n",
    "  y = torch.argmax(prediction, dim=1)\n",
    "  y = y.squeeze().cpu().numpy().astype(np.uint8)\n",
    "  \n",
    "  # populate accuracy table\n",
    "  score = y == gt_labels\n",
    "  scores.append(score)\n",
    "\n",
    "  img_labels = np.unique(gt_labels)\n",
    "  intersection, union = intersectionAndUnion(gt_labels, y, 35)\n",
    "  intersection = intersection[img_labels]\n",
    "  \n",
    "  union = union[img_labels]\n",
    "\n",
    "  # add a small number to union to avoid division by zero\n",
    "  iou = intersection / (union + 1e-10)\n",
    "  \n",
    "  # append IoU values to calculate mean IoU \n",
    "  ious.append(iou)\n",
    "\n",
    "  # populate array to calculate IoUs per class \n",
    "  for i in enumerate(img_labels):\n",
    "       labels_array[i[1],j] = iou[i[0]]\n",
    "  \n",
    "  j = j + 1 \n",
    "\n",
    "# convert to NumPy arrays \n",
    "scores = np.array(score)\n",
    "ious = np.array(ious)\n",
    "\n",
    "# calculate mean IoU\n",
    "for res in ious:\n",
    "  mean = np.mean(res)\n",
    "  means.append(mean)\n",
    "\n",
    "stop_time = time.time()\n",
    "execution_time = stop_time - start_time\n",
    "\n",
    "# print results\n",
    "print(\"Selected model: \", model_name, \"\\n\")\n",
    "print(\"Mean Accuracy: %.2f\\n\" %np.mean(scores))\n",
    "print(\"Mean IoU: %.2f\\n\" %np.mean(means))\n",
    "print(\"Execution time: %.4f seconds\\n\" %execution_time)\n",
    "\n",
    "# prints ids, labels and respective IoUs\n",
    "print(\"IoUs per label\\n\")\n",
    "for i in range(34):\n",
    "  print(\"Id:\", i, \"Label:\",id2label[i].name, \"IoU:\", np.around(np.nansum(labels_array[i,:], axis = 0, ) / np.count_nonzero(~np.isnan(labels_array[i,:])), decimals = 2))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e0K9eNWHxwEw"
   },
   "source": [
    "# Evaluation Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XU4zRojex5Wn"
   },
   "source": [
    "**FCN32 - ResNet50**\n",
    "\n",
    "Mean Accuracy: 0.77\n",
    "\n",
    "Mean IoU: 0.47\n",
    "\n",
    "Execution time: 105 seconds\n",
    "\n",
    "---\n",
    "**FCN32 - ResNet18**\n",
    "\n",
    "Mean Accuracy: 0.73\n",
    "\n",
    "Mean IoU: 0.47\n",
    "\n",
    "Execution time: 86 seconds\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**FCN16 - VGG**\n",
    "\n",
    "Mean Accuracy: 0.73\n",
    "\n",
    "Mean IoU: 0.50\n",
    "\n",
    "Execution time: 175 seconds\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**FCN8 - VGG**\n",
    "\n",
    "Mean Accuracy: 0.75\n",
    "\n",
    "Mean IoU: 0.51\n",
    "\n",
    "Execution time: 174 seconds\n",
    "\n",
    "\n",
    "---\n",
    "**SegNet**\n",
    "\n",
    "Mean Accuracy: 0.73\n",
    "\n",
    "Mean IoU: 0.40\n",
    "\n",
    "Execution time: 298 seconds\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "*for detailed IoUs per class please refer to project report screenshots\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Untitled0.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
