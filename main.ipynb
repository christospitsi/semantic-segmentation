{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5UffaGR1EfD",
        "colab_type": "text"
      },
      "source": [
        "# Path"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfMc3OKphLsO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# mount Google Drive every time you restart instance\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNDCnfmECMG0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set working path every time after mounting drive or set alternative path\n",
        "root_path = 'gdrive/My Drive/Dataset/'\n",
        "cityscapes_path = root_path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7NdV9_rx766",
        "colab_type": "text"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YY2SsKqD-P0W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function, division\n",
        "from distutils.version import LooseVersion\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.utils import data\n",
        "from torchvision import transforms, utils\n",
        "import cv2\n",
        "import os, sys, time\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from glob import glob\n",
        "from pathlib import Path\n",
        "from skimage import io, transform\n",
        "from tqdm import tqdm\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models\n",
        "from torchvision.models.vgg import VGG\n",
        "import argparse\n",
        "from PIL import Image\n",
        "\n",
        "from albumentations import (RandomContrast, RandomBrightness, Compose, RandomCrop, HorizontalFlip, Normalize)\n",
        "\n",
        "# import files for labels and models\n",
        "%run 'gdrive/My Drive/Colab Notebooks/labels.py'\n",
        "%run 'gdrive/My Drive/Colab Notebooks/models.py'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rF8fNAFBvnAX",
        "colab_type": "text"
      },
      "source": [
        "# Dataset Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fc_eAY3F-UkB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set username and password for Datascapes account. Download images\n",
        "# This code block runs only once to create dataset. Username and password have been removed. \n",
        "\n",
        "!wget --keep-session-cookies --save-cookies=cookies.txt --post-data 'username=XXXXXX&password=XXXXX&submit=Login' https://www.cityscapes-dataset.com/login/\n",
        "!wget --load-cookies cookies.txt --content-disposition https://www.cityscapes-dataset.com/file-handling/?packageID=3\n",
        "!wget --load-cookies cookies.txt --content-disposition https://www.cityscapes-dataset.com/file-handling/?packageID=1  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLABKQLC-XNT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create folders and unzip dataset\n",
        "\n",
        "!mkdir dataset && mkdir dataset/gtFine_trainvaltest && mkdir dataset/leftImg8bit_trainvaltest\n",
        "!unzip gtFine_trainvaltest.zip -d dataset/gtFine_trainvaltest\n",
        "!unzip leftImg8bit_trainvaltest.zip -d dataset/leftImg8bit_trainvaltest"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JWayeH26Tqg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Resize images to half the size\n",
        "\n",
        "images = list(Path(\"/content/dataset/leftImg8bit_trainvaltest/\").glob('**/*.png'))\n",
        "\n",
        "for image in images:\n",
        "   oriimg = cv2.imread(str(image), cv2.IMREAD_COLOR)\n",
        "   newimg = cv2.resize(oriimg, None, fx=0.5,fy=0.5, interpolation = cv2.INTER_AREA)\n",
        "   cv2.imwrite(str(image), newimg)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHYJlqsSxnNi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Resize annotations to half the size\n",
        "\n",
        "# Annotations = list(Path(\"/content/dataset/gtFine_trainvaltest/\").glob('**/*.png'))\n",
        "\n",
        "for annotation in annotations:\n",
        "   orianno = cv2.imread(str(annotation), cv2.IMREAD_GRAYSCALE)\n",
        "   newanno = cv2.resize(orianno, None, fx=0.5,fy=0.5, interpolation = cv2.INTER_NEAREST)\n",
        "   cv2.imwrite(str(annotation), newanno)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ue9A1JlcSWO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# copy resized images to Google Drive folder\n",
        "\n",
        "!cp -r dataset/ gdrive/My\\ Drive/Dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7udhvL04p9v",
        "colab_type": "text"
      },
      "source": [
        "# Image Transformations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOeI62v3_Zz0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# image transformations\n",
        "\n",
        "# crop should be changed to 1024 if original sized imaged are to be used \n",
        "\n",
        "trans_crop = RandomCrop(p=1, height=512, width=512)\n",
        "trans_flip = HorizontalFlip(p=0.5)\n",
        "trans_contrast = RandomContrast(p=0.1)\n",
        "trans_brightness = RandomBrightness(p=0.1)\n",
        "trans_norm = Normalize(p=1)\n",
        "\n",
        "# augmentations for training\n",
        "aug = Compose([trans_crop, trans_flip, trans_contrast, trans_norm])\n",
        "\n",
        "# augmentations for testing - only normalisation\n",
        "aug_test = Compose([trans_norm])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtT5pMHWgRW3",
        "colab_type": "text"
      },
      "source": [
        "# Some Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHl_2TU_Coj_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print coloured image prediction \n",
        "def labels2rgb(labels, lut):\n",
        "    im = cv2.LUT(cv2.merge((labels, labels, labels)), lut)\n",
        "    im = cv2.cvtColor(im, cv2.COLOR_RGB2BGR)\n",
        "    return im"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUvmkZLI_c2z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# one hot encoding\n",
        "def one_hot(anot, class_num):\n",
        "    eye_tb = np.eye(class_num)[np.array(anot).reshape(-1)]\n",
        "    return eye_tb.reshape(list(anot.shape)+[class_num])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KZq-OXpxQdw",
        "colab_type": "text"
      },
      "source": [
        "# Cityscape Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FYajChd_fiR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CityscapeDateset(Dataset):\n",
        "    \"\"\"Cityscape dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, root_dir, mode = \"train\",transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root_dir (string): Directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        \n",
        "        self.images_path = os.path.join(root_dir ,\"leftImg8bit_trainvaltest\")\n",
        "        self.annotations_path = os.path.join(root_dir , \"gtFine_trainvaltest\")\n",
        "\n",
        "        self.imgs_train = list(Path(self.images_path+'/leftImg8bit/train').glob('**/*.png'))\n",
        "        self.imgs_test = list(Path(self.images_path+'/leftImg8bit/test').glob('**/*.png'))\n",
        "        self.imgs_val = list(Path(self.images_path+'/leftImg8bit/val').glob('**/*.png'))\n",
        "        self.imgs_train = sorted(self.imgs_train)\n",
        "        self.imgs_test = sorted(self.imgs_test)\n",
        "        self.imgs_val = sorted(self.imgs_val)\n",
        "        \n",
        "        self.annotations_train = list(Path(self.annotations_path+'/gtFine/train').glob('**/*gtFine_labelIds.png'))\n",
        "        self.annotations_test = list(Path(self.annotations_path+'/gtFine/test').glob('**/*gtFine_labelIds.png'))\n",
        "        self.annotations_val = list(Path(self.annotations_path+'/gtFine/val').glob('**/*gtFine_labelIds.png'))\n",
        "        self.annotations_train = sorted(self.annotations_train)\n",
        "        self.annotations_test = sorted(self.annotations_test)\n",
        "        self.annotations_val = sorted(self.annotations_val)\n",
        "        \n",
        "        self.transform = transform\n",
        "        self.mode = mode\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.mode == \"train\":\n",
        "            return len(self.imgs_train)\n",
        "        elif self.mode == \"test\":\n",
        "            return len(self.imgs_test)\n",
        "        elif self.mode == \"val\":\n",
        "            return len(self.imgs_val)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.mode == \"train\":\n",
        "            img_name = self.imgs_train[idx]\n",
        "            annotation_name = self.annotations_train[idx]\n",
        "  \n",
        "        elif self.mode == \"test\":\n",
        "            img_name = self.imgs_test[idx]\n",
        "            annotation_name = self.annotations_test[idx]\n",
        "\n",
        "        elif self.mode == \"val\":\n",
        "            img_name = self.imgs_val[idx]\n",
        "            annotation_name = self.annotations_val[idx]\n",
        "\n",
        "        \n",
        "        image = io.imread(img_name)\n",
        "        annotations = io.imread(annotation_name)\n",
        "\n",
        "        if self.mode == \"train\":\n",
        "            augmented = aug(image=image, mask=annotations)\n",
        "          \n",
        "            image = torch.from_numpy(augmented['image']).float().permute(2, 0, 1)\n",
        "\n",
        "            annotations = augmented['mask']\n",
        "            w,h = annotations.shape\n",
        "            annotations = annotations.reshape(w*h,)\n",
        "            annotations = one_hot(annotations, 34)\n",
        "            annotations = annotations.reshape(w,h,34)\n",
        "\n",
        "            sample = {'image':image , 'annotation': annotations} \n",
        "        else:\n",
        "            augmented = aug_test(image=image, mask=annotations)\n",
        "            image = torch.from_numpy(augmented['image']).float().permute(2, 0, 1)\n",
        "            sample = {'image':image , 'annotation': annotations, \"name\": img_name}\n",
        "\n",
        "        return sample"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9d-UGC9X_ico",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining training and test sets\n",
        "# validation set used as test set \n",
        "\n",
        "train_db = CityscapeDateset(cityscapes_path)\n",
        "test_db = CityscapeDateset(cityscapes_path, mode = \"val\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvDxGUyv9xFn",
        "colab_type": "text"
      },
      "source": [
        "# Model/Mode Selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhZxY-7OR7UP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model_selection function asks the user to select the model to be trained \n",
        "\n",
        "# 1. FCN32 with ResNet50 backbone\n",
        "# 2. FCN32 with ResNet18 backbone\n",
        "# 3. FCN16 with VGG backbone\n",
        "# 4. FCN8 with VGG backbone\n",
        "# 5. SegNet\n",
        "\n",
        "while True:\n",
        "  selected_model = int(input(\"\"\"Please select model:\n",
        "\n",
        "  1. FCN32 with ResNet50 backbone\n",
        "  2. FCN32 with ResNet18 backbone\n",
        "  3. FCN16 with VGG backbone\n",
        "  4. FCN8 with VGG backbone\n",
        "  5. SegNet \\n\n",
        "  \"\"\"))\n",
        "\n",
        "  if selected_model == 1:\n",
        "      backbone = models.resnet50(pretrained=True)\n",
        "      backbone = torch.nn.Sequential(*(list(backbone.children())[:-2]))\n",
        "      model = FCN32s(backbone, 34, n_feats_backbone=2048).float().cuda()\n",
        "      model_name = \"fcn32_resnet50\"\n",
        "      print(\"\\nFCN32 with ResNet50 backbone selected.\")\n",
        "      break\n",
        "  elif selected_model == 2:\n",
        "      backbone = models.resnet18(pretrained=True)\n",
        "      backbone = torch.nn.Sequential(*(list(backbone.children())[:-2]))\n",
        "      model = FCN32s(backbone, 34, n_feats_backbone=512).float().cuda()\n",
        "      model_name = \"fcn32_resnet18\"\n",
        "      print(\"\\nFCN32 with ResNet18 backbone selected.\")\n",
        "      break\n",
        "  elif selected_model == 3:\n",
        "      vgg_model = VGGNet()\n",
        "      model = FCN16s_VGG(pretrained_net=vgg_model, n_class=34).float().cuda()\n",
        "      model_name = \"fcn16_vgg\"\n",
        "      print(\"\\nFCN16 with VGG backbone selected.\")\n",
        "      break\n",
        "  elif selected_model == 4:\n",
        "      vgg_model = VGGNet()\n",
        "      model = FCN8s_VGG(pretrained_net=vgg_model, n_class=34).float().cuda()\n",
        "      model_name = \"fcn8_vgg\"\n",
        "      print(\"\\nFCN8 with VGG backbone selected.\")\n",
        "      break\n",
        "  elif selected_model == 5:\n",
        "      model = segnet(3,34).float().cuda()\n",
        "      model_name = \"segnet\"\n",
        "      print(\"\\nSegNet network selected.\\n\")\n",
        "      break\n",
        "  else:\n",
        "      print(\"Please select a valid model number\\n\")\n",
        "\n",
        "# mode selection\n",
        "\n",
        "while True:\n",
        "  mode_sel = int(input(\"\"\"Please select mode:\n",
        "\n",
        "  1. Training\n",
        "  2. Testing/Evaluating\\n\n",
        "  \"\"\"))\n",
        "  if mode_sel == 1:\n",
        "      print(\"\\nProceed to training...\")\n",
        "      break\n",
        "\n",
        "  elif mode_sel == 2:\n",
        "    model_path = os.path.join(cityscapes_path, model_name + \".pt\")\n",
        "\n",
        "    if os.path.exists(model_path) == True:\n",
        "      checkpoint = torch.load(model_path)\n",
        "      model.load_state_dict(checkpoint['model_state_dict'])\n",
        "      break\n",
        "    else:\n",
        "      print(\"Saved model \\\"\",model_name,\"\\\" not found. Please train model first.\", sep='')\n",
        "  else:\n",
        "    print(\"Please select a valid mode number\\n\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTJsCgkT2Gr9",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5FNtmHWZsSP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_path = os.path.join(cityscapes_path, model_name + \".pt\")\n",
        "\n",
        "# define loss function and optimiser\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# model trained for 20 epochs. modify as appropriate\n",
        "max_epoch = 19\n",
        "\n",
        "dataloader = DataLoader(train_db, batch_size=4, shuffle=True, num_workers=4)\n",
        "\n",
        "if os.path.exists(model_path) == True:\n",
        "  checkpoint = torch.load(model_path)\n",
        "  start_epoch = int(checkpoint['epoch'])\n",
        "  model.load_state_dict(checkpoint['model_state_dict'])\n",
        "  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "  loss = checkpoint['loss']\n",
        "  \n",
        "  if start_epoch < max_epoch:\n",
        "    print(\"Loading epoch %d. Training resuming...\" %start_epoch)\n",
        "  else:\n",
        "    pring(\"Network fully trained\")\n",
        "else:\n",
        "  print(\"No checkpoint found. Training starting...\")\n",
        "  start_epoch = 0\n",
        "\n",
        "for epoch in range(start_epoch, max_epoch):\n",
        "  for i_batch, sample_batched in enumerate(tqdm(dataloader)):\n",
        "      model.train()\n",
        "      images = sample_batched['image'].float().cuda()\n",
        "      gt = sample_batched['annotation'].permute(0,3,1,2).float().cuda()\n",
        "      predictions = model(images)\n",
        "      loss = criterion(predictions, gt)\n",
        "\n",
        "      # Before the backward pass zero all gradients\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # Backward pass: compute gradient of the loss with respect to model parameters\n",
        "      loss.backward()\n",
        "\n",
        "      # Calling the step function on an Optimizer makes an update to its parameters\n",
        "      optimizer.step()\n",
        "\n",
        "  print(\"Loss: %f\" %loss.item())\n",
        "  print(\"Epoch: %d\" %epoch)\n",
        "\n",
        "  torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss\n",
        "}, model_path)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ThwbJhf96d2",
        "colab_type": "text"
      },
      "source": [
        "# Experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5oxvJm_x_NL",
        "colab_type": "code",
        "outputId": "352b5568-2771-4fe6-b438-fd325f137549",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(\"Selected model: \", model_name, \"\\n\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Selected model:  fcn32_resnet50 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TI08sDue9ll7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# go into evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# select a random image\n",
        "sample = test_db[2]\n",
        "\n",
        "img = sample[\"image\"].unsqueeze(0)\n",
        "prediction = model(img.cuda())\n",
        "\n",
        "# checks tensor dimensions for original image and predictions\n",
        "print(\"Image tensor:  \", img.size())\n",
        "print(\"Prediction tensor: \", prediction.size())\n",
        "\n",
        "prediction = torch.nn.functional.softmax(prediction,1)\n",
        "y = torch.argmax(prediction, dim=1)\n",
        "\n",
        "#now visualise all of them\n",
        "labels2color = np.zeros((256, 3), dtype=np.uint8)\n",
        "\n",
        "for lbl in labels:\n",
        "    labels2color[lbl.id] = np.array(lbl.color)\n",
        "labels2color = np.expand_dims(labels2color,0)\n",
        "\n",
        "y = y.squeeze().cpu().numpy().astype(np.uint8)\n",
        "\n",
        "# check if the size of the prediction is correct\n",
        "print(\"Size of sample predicted image:\", y.shape)\n",
        "\n",
        "path = sample['name']\n",
        "#print(path)\n",
        "\n",
        "# read original image\n",
        "original_img = cv2.imread(str(path))\n",
        "\n",
        "# generate coloured prediction\n",
        "cityscapes_rgb = labels2rgb(y, labels2color)\n",
        "\n",
        "# write original and annotation images \n",
        "cv2.imwrite(os.path.join(cityscapes_path + model_name + \"_original.png\"), original_img)\n",
        "cv2.imwrite(os.path.join(cityscapes_path + model_name + \"_masks.png\"), cityscapes_rgb)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Bin2zmYJIX0",
        "colab_type": "text"
      },
      "source": [
        "# Testing / **Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnrdYHQnJK89",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# intersectionAndUnion() function from MIT CSAIL Computer Vision github repo\n",
        "# https://github.com/CSAILVision/semantic-segmentation-pytorch\n",
        "\n",
        "def intersectionAndUnion(imPred, imLab, numClass):\n",
        "    imPred = np.asarray(imPred).copy()\n",
        "    imLab = np.asarray(imLab).copy()\n",
        "\n",
        "    imPred += 1\n",
        "    imLab += 1\n",
        "    # Remove classes from unlabeled pixels in gt image.\n",
        "    # We should not penalize detections in unlabeled portions of the image.\n",
        "    imPred = imPred * (imLab > 0)\n",
        "\n",
        "    # Compute area intersection:\n",
        "    intersection = imPred * (imPred == imLab)\n",
        "    (area_intersection, _) = np.histogram(\n",
        "        intersection, bins=numClass, range=(1, numClass))\n",
        "\n",
        "    # Compute area union:\n",
        "    (area_pred, _) = np.histogram(imPred, bins=numClass, range=(1, numClass))\n",
        "    (area_lab, _) = np.histogram(imLab, bins=numClass, range=(1, numClass))\n",
        "    area_union = area_pred + area_lab - area_intersection\n",
        "\n",
        "    return (area_intersection, area_union)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IaPIlrrRsm2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# go into evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# create tables to store scores IoUs and IoUs per class\n",
        "scores = []\n",
        "ious = []\n",
        "means = []\n",
        "\n",
        "#initialize with NaNs. NaNs will be present when a class does not exist in a picture and then ignored in calculations\n",
        "labels_array = np.empty((34,500)) * np.nan \n",
        "\n",
        "# counter for IoU per class array\n",
        "j = 0 \n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# predict all pictures\n",
        "for sample in test_db:\n",
        "  img = sample[\"image\"].unsqueeze(0)\n",
        "  gt_labels = sample[\"annotation\"]\n",
        "  prediction = model(img.cuda())\n",
        "  prediction = torch.nn.functional.softmax(prediction,1)\n",
        "  y = torch.argmax(prediction, dim=1)\n",
        "  y = y.squeeze().cpu().numpy().astype(np.uint8)\n",
        "  \n",
        "  # populate accuracy table\n",
        "  score = y == gt_labels\n",
        "  scores.append(score)\n",
        "\n",
        "  img_labels = np.unique(gt_labels)\n",
        "  intersection, union = intersectionAndUnion(gt_labels, y, 35)\n",
        "  intersection = intersection[img_labels]\n",
        "  \n",
        "  union = union[img_labels]\n",
        "\n",
        "  # add a small number to union to avoid division by zero\n",
        "  iou = intersection / (union + 1e-10)\n",
        "  \n",
        "  # append IoU values to calculate mean IoU \n",
        "  ious.append(iou)\n",
        "\n",
        "  # populate array to calculate IoUs per class \n",
        "  for i in enumerate(img_labels):\n",
        "       labels_array[i[1],j] = iou[i[0]]\n",
        "  \n",
        "  j = j + 1 \n",
        "\n",
        "# convert to NumPy arrays \n",
        "scores = np.array(score)\n",
        "ious = np.array(ious)\n",
        "\n",
        "# calculate mean IoU\n",
        "for res in ious:\n",
        "  mean = np.mean(res)\n",
        "  means.append(mean)\n",
        "\n",
        "stop_time = time.time()\n",
        "execution_time = stop_time - start_time\n",
        "\n",
        "# print results\n",
        "print(\"Selected model: \", model_name, \"\\n\")\n",
        "print(\"Mean Accuracy: %.2f\\n\" %np.mean(scores))\n",
        "print(\"Mean IoU: %.2f\\n\" %np.mean(means))\n",
        "print(\"Execution time: %.4f seconds\\n\" %execution_time)\n",
        "\n",
        "# prints ids, labels and respective IoUs\n",
        "print(\"IoUs per label\\n\")\n",
        "for i in range(34):\n",
        "  print(\"Id:\", i, \"Label:\",id2label[i].name, \"IoU:\", np.around(np.nansum(labels_array[i,:], axis = 0, ) / np.count_nonzero(~np.isnan(labels_array[i,:])), decimals = 2))\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0K9eNWHxwEw",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XU4zRojex5Wn",
        "colab_type": "text"
      },
      "source": [
        "**FCN32 - ResNet50**\n",
        "\n",
        "Mean Accuracy: 0.77\n",
        "\n",
        "Mean IoU: 0.47\n",
        "\n",
        "Execution time: 105 seconds\n",
        "\n",
        "---\n",
        "**FCN32 - ResNet18**\n",
        "\n",
        "Mean Accuracy: 0.73\n",
        "\n",
        "Mean IoU: 0.47\n",
        "\n",
        "Execution time: 86 seconds\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**FCN16 - VGG**\n",
        "\n",
        "Mean Accuracy: 0.73\n",
        "\n",
        "Mean IoU: 0.50\n",
        "\n",
        "Execution time: 175 seconds\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**FCN8 - VGG**\n",
        "\n",
        "Mean Accuracy: 0.75\n",
        "\n",
        "Mean IoU: 0.51\n",
        "\n",
        "Execution time: 174 seconds\n",
        "\n",
        "\n",
        "---\n",
        "**SegNet**\n",
        "\n",
        "Mean Accuracy: 0.73\n",
        "\n",
        "Mean IoU: 0.40\n",
        "\n",
        "Execution time: 298 seconds\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "*for detailed IoUs per class please refer to project report screenshots\n",
        "\n"
      ]
    }
  ]
}